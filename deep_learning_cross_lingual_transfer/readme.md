# Bilingual models for better cross-lingual transfer

The main goals of this project were:

- Evaluate the performance of State-of-the-Art natural language processing neural networks (mBert and BERTimbau) when fine-tuned in different languages from the pre-train language;
- Evaluate if there are impacts on performance when a distant language (Vietnamese) is used to fine-tune neural networks for tasks in Portuguese;
- Measure the correlation between the size of the datasets used in the fine-tuning and the performance of neural networks when tested in other languages.

For further details of the implementation and the result – please check the pdf document and the notebook.

**Please note that not all the codebase was writen by me – most of the boilerplate code was pre-writen**